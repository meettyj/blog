---
layout: post
title:  Nutch
category: jekyll
description: The process in Nutch using Map/Reduce.
---

<br />

# 一、Nutch-0.9 在Eclipse 上的配置
首先我们要将nutch下载下来，解压得到nutch的文件夹后便可在Eclipse中对相应的配置文件进行修改。具体操作如下：
- 新建一个Java 项目：
打开Eclipse，点击“File”，依次选择“New”、“Java Project”新建一个Java 项目，并将新建的项目命名为Nutch。

<div align='center'>
<img src="{{site.baseurl}}/assets/img/nutch/1.png" alt="nutch-1"/></div>

- 导入Nutch 源码：
将解压出来的“nutch-0.9\src\java”目录下的“org”文件夹整个复制到项目文件夹“Nutch”的“src”目录下：

<div align='center'>
<img src="{{site.baseurl}}/assets/img/nutch/2.png" alt="nutch-2"/>（1）</div>
<div align='center'>
<img src="{{site.baseurl}}/assets/img/nutch/3.png" alt="nutch-3"/>（2）</div>

- 将“nutch-0.9”目录下的“conf”、“lib”、“plugins”文件夹复制到根目录下，并在“conf”文件夹下右键选择“构建路径”->“用作源文件夹”：

<div align='center'>
<img src="{{site.baseurl}}/assets/img/nutch/4.png" alt="nutch-4"/></div>

- 在Nutch2项目名上右键选择“构建路径”->“配置构建路径”->“库”->“添加外部JAR”，将lib文件夹中的所有JAR包添加到库中，截图如下：

<div align='center'>
<img src="{{site.baseurl}}/assets/img/nutch/5.png" alt="nutch-5"/></div>

- 然后我们需要修改“conf”目录下的“nutch-site.xml”文件，添加如下代码：

`<property><name>http.agent.name</name><value>*</value></property>`

添加完后截图如下：
<div align='center'>
<img src="{{site.baseurl}}/assets/img/nutch/6.png" alt="nutch-6"/></div>

- 然后修改“conf”目录下的“crawl-urlfilter.txt”文件，将其中的：<br />
#accept hosts in MY.DOMAIN.NAME<br />
+^http://([a-z0-9]*\.)*MY.DOMAIN.NAME/<br />
修改为：<br />
#accept hosts in MY.DOMAIN.NAME<br />
+^http://([a-z0-9]*\.)*view.sdu.edu.cn/<br />
即表示nutch的爬虫将只会在view.sdu.edu.cn中进行爬取，即实验要求中的山东大学新闻网。
截图如下：

<div align='center'>
<img src="{{site.baseurl}}/assets/img/nutch/7.png" alt="nutch-7"/></div>

- 在根目录下新建名为“urls”的文件夹，并在其中新建一个名为“urls”的txt 文件， 在其中写入要爬取的url地址， 如http://www.view.sdu.edu.cn/，截图如下：

<div align='center'>
<img src="{{site.baseurl}}/assets/img/nutch/8.png" alt="nutch-8"/></div><br />

其会在后面输入爬虫命令的时候用到，我将会在后面进行详细说明。

- 接下来nutch的配置就基本完成了，我们可以运行一下爬虫crawl命令来对相关内容进行爬取了。
首先找到Crawl.java类，并对其进行“运行配置”，相关截图如下：

<div align='center'>
<img src="{{site.baseurl}}/assets/img/nutch/9.png" alt="nutch-9"/>（1）</div>
<div align='center'>
<img src="{{site.baseurl}}/assets/img/nutch/10.png" alt="nutch-10"/>（2）</div>

其中 crawl urls -dir result -threads 20 -depth 3 命令中分别对应的意思如下：<br />
1. crawl: nutch 的爬虫命令。<br />
1. urls: 之前建立的 urls 文件夹，用来读取需要爬取的网址。<br />
1. -dir result：将爬取结果输出到命名为result的文件夹中。<br />
1. -threads 20:开启的进程数为20。<br />
1. -depth 3:要爬取得深度为3。默认是5。<br />
1. -topN 100: 设置从任何一个网页获取的最大外链数为100。<br />
1. -Xms[初始化堆内存大小] <br />
1. -Xmx[可以使用的最大堆内存]<br /><br />

- 点击“运行”，便可在控制台界面看到如下的输出结果：
<div align='center'>
<img src="{{site.baseurl}}/assets/img/nutch/11.png" alt="nutch-11"/>（1）</div>
<div align='center'>
<img src="{{site.baseurl}}/assets/img/nutch/12.png" alt="nutch-12"/>（2）</div>
<div align='center'>
<img src="{{site.baseurl}}/assets/img/nutch/13.png" alt="nutch-13"/>（3）</div>
<div align='center'>
<img src="{{site.baseurl}}/assets/img/nutch/14.png" alt="nutch-14"/>（4）</div>

- 然后我们可以看到在我们的路径下已经有了命名为result的文件夹，截图如下：
<div align='center'>
<img src="{{site.baseurl}}/assets/img/nutch/15.png" alt="nutch-15"/></div><br />

下面给出result文件夹中各子文件夹的对象说明：
1. Nutch的数据文件
1. crawldb: 爬行数据库，用来存储所要爬行的网址
1. linkdb: 链接数据库，用来存储每个网址的链接地址，包括源地址和链接地址
1. segments: 抓取的网址被作为一个单元，而一个segment就是一个单元。一个segment包括以下子目录:
- crawl_generate:包含所抓取的网址列表
- crawl_fetch:包含每个抓取页面的状态
- content:包含每个抓取页面的内容
- parse_text:包含每个抓取页面的解析文本
- parse_data:包含每个页面的外部链接和元数据
- crawl_parse:包含网址的外部链接地址，用于更新crawldb数据库
1. indexes: 采用Lucene的格式建立索引集
1. index：最终生成的目录
<br /><br />

















# 二、Nutch Map/Reduce Process
1. 插入url列表到Crawl DB，引导下面的抓取程序
2. 循环:
- 从Crawl DB生成一些url列表;
- 抓取内容;
- 分析处理抓取的内容;
- 更新Crawl DB库.
3. 转化每个页面中外部对它的链接
4. 建立索引

<br />

## Ⅰ.Inject(插入url列表)
1. MapReduce程序1: 
- 目标:转换input输入为CrawlDatum格式. 
- 输入: url文件 
- Map(line) → <url, CrawlDatum> 
- Reduce()合并多重的Url.
- 输出:临时的CrawlDatum文件. 
2. MapReduce2: 
- 目标:合并上一步产生的临时文件到新的DB 
- 输入: 上次MapReduce输出的CrawlDatum 
- Map()过滤重复的url. 
- Reduce: 合并两个CrawlDatum到一个新的DB 
- 输出:CrawlDatum
<br /><br />

## Ⅱ.Generate(生成抓取列表)
1. MapReduce程序1: 
- 目标:选择抓取列表 
- 输入: Crawl DB 文件 
- Map() → 如果抓取当前时间大于现在时间 ,抓换成 <CrawlDatum, url>格式. 
- 分发器(Partition) :用url的host保证同一个站点分发到同一个Reduce程序上. 
- Reduce:取最顶部的N个链接. 
2. MapReduce程序2: 
- 目标:准备抓取 
- Map() 抓换成 <url,CrawlDatum,>格式 
- 分发器(Partition) :用url的host 
- 输出:<url,CrawlDatum>文件
<br /><br />

## Ⅲ.Fetch(抓取内容)
1. MapReduce: 
- 目标:抓取内容 
- 输入: <url,CrawlDatum>, 按host划分, 按hash排序 
- Map(url,CrawlDatum) → 输出<url, FetcherOutput> 
- 多线程, 调用Nutch的抓取协议插件,抓取输出<CrawlDatum, Content> 
- 输出: <url,CrawlDatum>, <url,Content>两个文件
<br /><br />

## Ⅳ.Parse(分析处理内容)
1. MapReduce: 
- 目标:处理抓取的能容 
- 输入: 抓取的<url, Content> 
- Map(url, Content) → <url, Parse> 
- 调用Nutch的解析插件,输出处理完的格式是<ParseText, ParseData> 
- 输出: <url,ParseText>, <url,ParseData><url,CrawlDatum>.
<br /><br />

## Ⅴ.Update(更新Crawl DB库)
1. MapReduce: 
- 目标: 整合 fetch和parse到DB中 
- 输入:<url,CrawlDatum> 现有的db加上fetch和parse的输出,合并上面3个DB为一个新的DB 
- 输出: 新的抓取DB
<br /><br />

## Ⅵ.Invert Links(转化链接)
1. MapReduce: 
- 目标:统计外部页面对本页面链接 
- 输入: <url,ParseData>, 包含页面往外的链接 
- Map(srcUrl, ParseData> → <destUrl, Inlinks> 
- 搜集外部对本页面的链接Inlinks格式:<srcUrl, anchorText> 
- Reduce() 添加inlinks 
- 输出: <url, Inlinks>
<br /><br />

## Ⅶ.Index(建立索引)
1. MapReduce:
- 目标:生成Lucene索引
- 输入: 多种文件格式
- parse处理完的<url, ParseData> 提取title, metadata信息等
- parse处理完的<url, ParseText> 提取text内容
- 转换链接处理完的<url, Inlinks> 提取anchors
- 抓取内容处理完的<url, CrawlDatum> 提取抓取时间
- Map() 用ObjectWritable包裹上面的内容
- Reduce() 调用Nutch的索引插件,生成Lucene Document文档
- 输出: 输出Lucene索引
<br /><br />